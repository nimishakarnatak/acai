<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ACAI for SBOs: AI Co-creation for Advertising and Inspiration for Small Business Owners</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">ACAI for SBOs: AI Co-creation for Advertising and Inspiration for Small Business Owners</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://www.nimishakarnatak.com/">Nimisha Karnatak</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://research.google/people/108123/">Adrien Baranes</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.rob-marchant.com/">Rob Merchant</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.trionabutler.com/about">Tríona Butler</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://research.google/people/106948/">Kristen Olson</a>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Work done at Google Deepmind, London.</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a target="_blank" href="assets/pdfs/paper.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                    <a target="_blank" href="https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-blog"></i>
                                        </span>
                                        <span>Poster</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-widescreen">
            <div class="column content">
                <video poster="" autoplay muted loop height="100%" width="100%">
                    <source src="assets/videos/open-eqa-teaser.mp4" type="video/mp4">
                </video>
            </div>
            <h2 class="subtitle has-text-centered">
                Illustration of an episode history along with questions and answers from our <b>OpenEQA
                    benchmark</b>,
                which contains 1600+ untemplated questions that tests several aspects of open vocabulary embodied question answering.
            </h2>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 110%">
                            We present a modern formulation of Embodied Question Answering (EQA) as the task of understanding an environment well enough to answer questions about it in natural language. An agent can achieve such an understanding by either drawing upon episodic memory, exemplified by agents on smart glasses, or by actively exploring the environment, as in the case of mobile robots. We accompany our formulation with OpenEQA - the first open-vocabulary benchmark dataset for EQA supporting both episodic memory and active exploration use cases. OpenEQA contains over 1600 high-quality human generated questions drawn from over 180 real-world environments. In addition to the dataset, we also provide an automatic LLM-powered evaluation protocol that has excellent correlation with human judgement. Using this dataset and evaluation protocol, we evaluate several state-of-the-art foundation models like GPT-4V and find that they significantly lag behind human-level performance.  Consequently, OpenEQA stands out as a straightforward, measurable, and practically relevant benchmark that poses a considerable challenge to current generation of AI models. We hope this inspires and stimulates future research at the intersection of Embodied AI, conversational agents, and world models.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column">
                    <h2 class="title is-3">Performance of State-of-the-Art Models</h2>
                    <div class="columns is-centered">
                        <div class="column content">

                            <img src="assets/images/multi-modal-llms.svg" class="interpolation-image" alt=""
                                style="display: block; margin-left: auto; margin-right: auto" width="80%">
                            <span style="font-size: 110%">
                                <span style="font-weight: bold">
                                    <br />
                                    LLM vs. Multi-Modal LLM Performance on EM-EQA. </span>
                                    We evaluated several multi-modal LLMs including Claude 3, Gemini Pro, and GPT-4V
                                    on OpenEQA. We find that these models consistently outperform text-only (or blind)
                                    LLM baselines such as LLaMA-2 or GPT-4. However, performance is substantially worse
                                    than the human baselines.
                            </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvc1">OpenEQA Dataset Statistics</span></h2>
                        <img src="assets/images/figure-3.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" width="100%">
                        <span style="font-size: 110%">
                            <span style="font-weight: bold">
                                <br />
                                Example questions and dataset statistics of OpenEQA.</span> The episode history <i>H</i>
                            provides a human-like tour of a home. EQA agents must answer diverse, human-generated
                            questions <i>Q</i> from 7 EQA categories, aiming match the ground answers <i>A*</i>. Tours
                            are collected from diverse environments including home and office locations (not shown
                            above).
                            Dataset statistics (right) break down the question distribution by video source (top),
                            question category (middle), and
                            episodic memory vs active setting. Note that, by design, the HM3D questions are shared
                            across the EM-EQA and A-EQA settings.
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">

                <div class="column">
                    <h2 class="title is-3">Automated Evaluation Workflow</h2>
                    <div class="columns is-centered">
                        <div class="column content">

                            <img src="assets/images/workflow.png" class="interpolation-image" alt=""
                                style="display: block; margin-left: auto; margin-right: auto" width="100%">
                            <span style="font-size: 110%">
                                <span style="font-weight: bold">
                                    <br />
                                    <br />
                                    Illustration of LLM-Match evaluation and workflow. </span>
                                While the open-vocabulary nature makes EQA realistic, it poses a challenge for
                                evaluation
                                due to multiplicity of correct answers. One approach to evaluation is human trials, but
                                it
                                can be prohibitively slow and expensive, especially for benchmarks. As an alternative,
                                we
                                use an LLM to evaluate the correctness of open-vocabulary answers produced by EQA
                                agents.
                            </span>
                        </div>

                    </div>
                </div>
                <div class="column">
                    <h2 class="title is-3">Performance by Category</h2>
                    <div class="columns is-centered">
                        <div class="column content">

                            <img src="assets/images/spider-plot.svg" class="interpolation-image" alt=""
                                style="display: block; margin-left: auto; margin-right: auto" width="75%">
                            <span style="font-size: 110%">
                                <span style="font-weight: bold">
                                    <br />
                                    Category-level performance on EM-EQA. </span>
                                We find that agents with access to visual information excel at localizing and
                                recognizing objects and attributes, and make better use of this information to answer
                                questions that require world knowledge. However, on other categories performance is
                                closer to the blind LLM baseline (GPT-4), indicating
                                substantial room for improvement on OpenEQA.
                            </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
    @inproceedings{acai,
        title         = {ACAI for SBOs: AI Co-creation for Advertising and Inspiration for Small Business Owners}, 
        booktitle     = {Google DeepMind},
        author        = {Karnatak, Nimisha and Baranes, Adrien and Marchant, Rob and Butler, Tríona and Olson, Kristen},
        year          = {2025},
    }
        </code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                                href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
